# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

AI Chat is a lightweight OpenAI-compatible API chat frontend with user management and invite code registration. It acts as a proxy/middleware between users and any OpenAI-compatible API provider (OpenAI, Claude via OpenRouter, Gemini, local models, etc.).

**Key Architecture**: Single-file backend (`main.py`) + vanilla JavaScript frontend (`static/`) with no build step required.

## Development Commands

### Running the Application

```bash
# Install dependencies (uses uv package manager)
uv sync

# Start development server with auto-reload
uv run uvicorn main:app --reload --host 0.0.0.0 --port 8000

# Run in background for testing
uv run uvicorn main:app --reload --host 0.0.0.0 --port 8000 &
```

### Docker Operations

```bash
# Build for amd64 architecture
docker buildx build --platform linux/amd64 -t xichun/ai-chat:latest --load .

# Run with Docker Compose
docker compose up -d

# View logs
docker compose logs -f
```

### Testing

No automated tests currently. Manual testing workflow:
1. Start server locally
2. Test authentication flow (login/register with invite codes)
3. Test chat with different models
4. Test admin panel (invite codes, user management, API settings)
5. Test on mobile devices (iOS Safari, Android Chrome)

## Architecture

### Backend Structure (`main.py` - 489 lines)

**Single FastAPI application with clear sections:**

1. **Configuration** (lines 25-39): Environment variables and paths
2. **Data Storage** (lines 45-101): JSON file-based persistence in `data/` directory
   - `users.json`: User accounts with password hashes
   - `invite_codes.json`: Registration invite codes
   - `settings.json`: API configuration (URL and key)
3. **Authentication** (lines 184-205): JWT-based auth with 30-day tokens
4. **API Routes**:
   - Public: `/api/login`, `/api/register`
   - User: `/api/me`, `/api/models`, `/api/chat`
   - Admin: `/api/admin/*` (invite codes, users, settings)

**Critical Design Patterns:**

- **Dynamic API Configuration**: API URL and key can be changed at runtime via admin panel (stored in `settings.json`)
- **Streaming Proxy**: `/api/chat` streams responses from upstream API using `StreamingResponse`
- **First User is Admin**: The first user in `USERS` env var automatically gets admin privileges
- **Invite Code System**: Registration requires a valid, unused invite code generated by admin

### Frontend Structure (`static/` - 4,573 total lines)

**Three files, no build process:**

- `index.html` (16,407 lines): Single-page app structure with inline SVG icons
- `app.js` (1,769 lines): Vanilla JavaScript with clear sections:
  - State management (localStorage for conversations, JWT token)
  - Model selection with vendor grouping (Claude, GPT, Gemini, GLM)
  - Streaming chat with SSE parsing
  - Image upload support (base64 encoding)
  - **Base64 Image Detection**: Automatically detects and renders images in chat responses (for models like `gemini-2.5-flash-image-preview` that return base64 images)
  - Admin panel management
- `style.css` (2,315 lines): Comprehensive mobile-first responsive design
  - Dark/light theme support with CSS variables
  - iOS safe area handling (`env(safe-area-inset-*)`)
  - Touch-optimized (44px minimum touch targets)
  - Keyboard-aware layouts

**Key Frontend Patterns:**

- **Conversation Storage**: All chat history stored in `localStorage` (client-side only)
- **Model Grouping**: Models automatically grouped by vendor (Claude, GPT, Gemini, GLM, Other)
- **Streaming Display**: Real-time markdown rendering during streaming responses
- **Image Support**:
  - Upload images for vision models (base64 encoded)
  - Automatic detection of base64 images in responses (for image generation models)
  - Supports `data:image/*;base64,*` and pure base64 strings

## Data Flow

### Chat Request Flow

```
User Input → Frontend (app.js)
  → POST /api/chat with JWT token
    → Backend validates token
      → Loads API config from settings.json
        → Proxies to upstream API (OpenAI-compatible)
          → Streams response back to frontend
            → Frontend renders markdown + detects base64 images
```

### Settings Update Flow

```
Admin Panel → PUT /api/admin/settings
  → Backend saves to data/settings.json
    → Next chat request uses new settings (no restart needed)
```

## Important Implementation Details

### Image Generation Model Support

The frontend automatically detects and renders base64-encoded images in chat responses. This enables support for models that return images through the chat API (like `gemini-2.5-flash-image-preview`) rather than dedicated image generation endpoints.

**Detection Logic** (`app.js`):
- Detects `data:image/[format];base64,[data]` patterns
- Detects pure base64 strings and validates image file headers (PNG: `89 50 4E 47`, JPEG: `FF D8 FF`, WebP: `52 49 46 46`)
- Optimizes streaming performance by reducing render frequency for large base64 data

### Mobile Optimization

The app is production-ready for mobile with:
- **iOS-specific**: 16px input font size (prevents auto-zoom), safe area insets, `-webkit-overflow-scrolling: touch`
- **Touch feedback**: Scale animations on button press
- **Keyboard handling**: Auto-scroll to prevent input obstruction
- **Responsive breakpoints**: 768px for tablet/desktop switch

### Security Considerations

- **JWT tokens**: 30-day expiration, stored in localStorage
- **Password hashing**: SHA256 with SECRET_KEY salt (not bcrypt - consider upgrading for production)
- **API key storage**: Stored in plaintext in `data/settings.json` (ensure proper file permissions)
- **No rate limiting**: Consider adding for production use

## Common Modifications

### Adding a New API Endpoint

1. Define Pydantic model in `main.py` (around line 160)
2. Add route handler with `@app.post/get/put/delete`
3. Use `Depends(verify_token)` for auth or `Depends(verify_admin)` for admin-only
4. Update frontend `app.js` to call the new endpoint

### Supporting New Image Generation Models

The base64 image detection is already implemented. To support new models:
1. Ensure the model returns base64 image data in chat responses
2. The frontend will automatically detect and render images
3. No backend changes needed (it's a transparent proxy)

### Changing Authentication

Current: JWT with 30-day expiration
- Token creation: `create_access_token()` (line 194)
- Token validation: `verify_token()` (line 198)
- Admin check: `verify_admin()` (line 201)

### Adding New Data Storage

Follow the pattern in lines 45-96:
1. Define file path in `DATA_DIR`
2. Create `load_*()` and `save_*()` functions
3. Initialize in `init_data_dir()`

## Environment Variables

Required for production:
- `SECRET_KEY`: JWT signing key (generate with `openssl rand -hex 32`)
- `USERS`: Initial admin user (format: `username:password`)

Optional (can be set via admin panel):
- `API_BASE_URL`: OpenAI-compatible API endpoint
- `API_KEY`: API authentication key

## Deployment Notes

- **Docker**: Multi-stage build with Python 3.13, runs on port 8000
- **Data persistence**: Mount `./data` volume to preserve users, settings, invite codes
- **No database**: All data in JSON files (suitable for small-scale deployments)
- **Reverse proxy recommended**: Use Nginx/Caddy for HTTPS termination

## Known Limitations

- No database (JSON files only) - not suitable for high-concurrency scenarios
- No rate limiting on API endpoints
- Password hashing uses SHA256 (consider bcrypt for production)
- No automated tests
- Client-side conversation storage only (no server-side history)
- API key stored in plaintext in `settings.json`

## 何时提交git
- 只有当用户指明的时候才提交